{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMls0z5L1ClzsEoGySw91k9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunalsonalkar/training-llms/blob/main/fineTuning_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL_APWl0eviE",
        "outputId": "6060ab82-9639-4cd3-8d56-063151bd87b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLMs-from-scratch'...\n",
            "remote: Enumerating objects: 5555, done.\u001b[K\n",
            "remote: Total 5555 (delta 0), reused 0 (delta 0), pack-reused 5555 (from 1)\u001b[K\n",
            "Receiving objects: 100% (5555/5555), 12.92 MiB | 9.54 MiB/s, done.\n",
            "Resolving deltas: 100% (3479/3479), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rasbt/LLMs-from-scratch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    labels = pd.read_csv('/content/LLMs-from-scratch/ch06/01_main-chapter-code/query_intent_labels.csv', encoding='latin1',usecols=[0,1])\n",
        "except UnicodeDecodeError:\n",
        "    labels = pd.read_csv('/content/LLMs-from-scratch/ch06/01_main-chapter-code/query_intent_labels.csv', encoding='cp1252', usecols=[0,1])\n",
        "# If none of the encoding above work, you may also try:\n",
        "# labels = pd.read_csv('/content/LLMs-from-scratch/ch06/01_main-chapter-code/query_intent_labels.csv', encoding='utf-16')\n",
        "# labels = pd.read_csv('/content/LLMs-from-scratch/ch06/01_main-chapter-code/query_intent_labels.csv', encoding='iso-8859-1')"
      ],
      "metadata": {
        "id": "z8fGFIfmmZYz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Your category-to-integer mapping\n",
        "category_map = {\n",
        "    'baby_accessories_a28': 0,\n",
        "    'bags_a17': 1,\n",
        "    'belts__braces_a26': 2,\n",
        "    'bottoms_a3': 3,\n",
        "    'dresses_a1': 4,\n",
        "    'eyewear_a19': 5,\n",
        "    'food_a38': 6,\n",
        "    'fragrance_a31': 7,\n",
        "    'gloves_mittens_a21': 8,\n",
        "    'hair_accessories_a20': 9,\n",
        "    'hair_care_a33': 10,\n",
        "    'headwear_a18': 11,\n",
        "    'home_a35': 12,\n",
        "    'hosiery_a16': 13,\n",
        "    'jacket_sportcoat_mg12': 14,\n",
        "    'jewelry_a14': 15,\n",
        "    'jewelry_care_a15': 16,\n",
        "    'jumpsuits_coveralls_a4': 17,\n",
        "    'makeup_a30': 18,\n",
        "    'neckwear_a27': 19,\n",
        "    'outerwear_a11': 20,\n",
        "    'personal_care_accessories_a34': 21,\n",
        "    'scarves_wraps_ponchos_a22': 22,\n",
        "    'shoe_care_a13': 23,\n",
        "    'shoes_a12': 24,\n",
        "    'skin_body_treatment_a32': 25,\n",
        "    'sleepwear_a9': 26,\n",
        "    'small_leather_goods_a23': 27,\n",
        "    'stationery_giftwrap_a36': 28,\n",
        "    'suits_sets_wardrobers_mg5': 29,\n",
        "    'swimwear_a8': 30,\n",
        "    'toys_games_a37': 31,\n",
        "    'tops_a2': 32,\n",
        "    'underwear_lingerie_a7': 33\n",
        "}\n",
        "\n",
        "\n",
        "# Map categories to integers\n",
        "labels['category_int'] = labels['product_type_1_label'].map(category_map)\n",
        "print(labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoTeu4wantKV",
        "outputId": "06075342-7b06-48d0-8ee0-e7d6e264431b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              search_term     product_type_1_label  category_int\n",
            "0             womens uggs                shoes_a12            24\n",
            "1       charlotte tilbury               makeup_a30            18\n",
            "2     womens birkenstocks                shoes_a12            24\n",
            "3                    uggs                shoes_a12            24\n",
            "4            womens nikes                shoes_a12            24\n",
            "...                   ...                      ...           ...\n",
            "3684        girls pajamas             sleepwear_a9            26\n",
            "3685          hand towels                 home_a35            12\n",
            "3686            hand wash  skin_body_treatment_a32            25\n",
            "3687       l'agence jeans               bottoms_a3             3\n",
            "3688             lipgloss               makeup_a30            18\n",
            "\n",
            "[3689 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDrfSNKHVBMt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_split(df, train_frac=0.7, validation_frac=0.2):\n",
        "  df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "  train_end = int(len(df) * train_frac)\n",
        "  validation_end = train_end + int(len(df) * validation_frac)\n",
        "  train_df = df[:train_end]\n",
        "  validation_df = df[train_end:validation_end]\n",
        "  test_df = df[validation_end:]\n",
        "  return train_df, validation_df, test_df"
      ],
      "metadata": {
        "id": "i2Pz_4ugppzJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, validation_df, test_df = random_split(labels, train_frac=0.7, validation_frac=0.1)"
      ],
      "metadata": {
        "id": "IRKjQFIQspz8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv('train.csv', index=None)\n",
        "validation_df.to_csv('validation.csv', index=None)\n",
        "test_df.to_csv('test.csv', index=None)"
      ],
      "metadata": {
        "id": "wen-xCKWT2hD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nsbu8GHsITj",
        "outputId": "43549baf-d2e9-4f88-8e96-b586ff025a6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "print(tokenizer.decode([50256]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBCUd6yIxYEE",
        "outputId": "088ffdd1-cad0-4bb3-91a3-f556a90e986e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class queryIntent(Dataset):\n",
        "  def __init__(self, csv_file, tokenizer, max_length = None, pad_token_id=50256):\n",
        "      self.data = pd.read_csv(csv_file)\n",
        "      self.encoded_texts = [\n",
        "          tokenizer.encode(text) for text in self.data[\"search_term\"]\n",
        "      ]\n",
        "      if max_length is None:\n",
        "        self.max_length = self._longest_encoded_length()\n",
        "      else:\n",
        "        self.max_length = max_length\n",
        "        self.encoded_texts = [\n",
        "            encoded_text[:self.max_length] for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "      self.encoded_texts = [\n",
        "          encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "          for encoded_text in self.encoded_texts\n",
        "      ]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      encoded = self.encoded_texts[index]\n",
        "      label = self.data.iloc[index][\"category_int\"]\n",
        "      return (\n",
        "          torch.tensor(encoded, dtype=torch.long),\n",
        "          torch.tensor(label, dtype=torch.long)\n",
        "      )\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def _longest_encoded_length(self):\n",
        "      max_length = 0\n",
        "      for encoded_text in self.encoded_texts:\n",
        "          encoded_length = len(encoded_text)\n",
        "          if encoded_length > max_length:\n",
        "              max_length = encoded_length\n",
        "      return max_length"
      ],
      "metadata": {
        "id": "MgWut2U_yTOM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = queryIntent(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
        "validation_dataset = queryIntent(\"validation.csv\", max_length = None, tokenizer=tokenizer)\n",
        "test_dataset = queryIntent(\"test.csv\", max_length = None, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "NgC2Zi7lQ8gd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "validation_loader = DataLoader(\n",
        "    dataset=validation_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False\n",
        ")"
      ],
      "metadata": {
        "id": "1lNFUzp8VK5w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRC1POoVV2mC",
        "outputId": "757fdc10-f776-43c0-be68-62f4f3007b94"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "322"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYimi-dIWXhx",
        "outputId": "fd64e6bb-b5a9-4eec-a585-2afc7df65d29"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(validation_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xhiqYrJWmrf",
        "outputId": "105c63cf-e1d3-403d-b217-eeac70aa3964"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8usx47mAWpA6",
        "outputId": "d56f9cde-5593-4ad6-e602-8c5b675e791a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2582"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_batch, label_batch in train_loader:\n",
        "    print(input_batch.shape)\n",
        "    print(label_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXtdAf7fWx9a",
        "outputId": "ecf0f4da-51fa-4442-f9ab-eb8f7da46da0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 11])\n",
            "torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the model and Pretrained Weights**"
      ],
      "metadata": {
        "id": "oibgbzNyZXeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"drop_rate\": 0.0,\n",
        "    \"qkv_bias\": True,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_layers\": 12,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_embd\": 768\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\n",
        "        \"n_layer\": 12,\n",
        "        \"n_head\": 12,\n",
        "        \"n_embd\": 768\n",
        "    },\n",
        "    \"gpt2-medium (355M)\": {\n",
        "        \"n_layer\": 24,\n",
        "        \"n_head\": 16,\n",
        "        \"n_embd\": 1024\n",
        "    },\n",
        "    \"gpt2-large (774M)\": {\n",
        "        \"n_layer\": 36,\n",
        "        \"n_head\": 20,\n",
        "        \"n_embd\": 1280\n",
        "    },\n",
        "    \"gpt2-xl (1.5B)\": {\n",
        "        \"n_layer\": 48,\n",
        "        \"n_head\": 25,\n",
        "        \"n_embd\": 1600\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "7zm42uDRY0E8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLMs-from-scratch/ch06/01_main-chapter-code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNUpoLldb4Rk",
        "outputId": "ad51e4da-531b-49c4-bd80-6dd5f10dd936"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLMs-from-scratch/ch06/01_main-chapter-code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt_download\n",
        "from gpt_download import download_and_load_gpt2"
      ],
      "metadata": {
        "id": "8-mXkrhFbKk6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from previous_chapters import GPTModel, load_weights_into_gpt"
      ],
      "metadata": {
        "id": "YkgoCEf_byXs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir = \"gpt2\")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NbYyiYfb9zt",
        "outputId": "6754e347-0847-45c2-d245-43800787c7bc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 102kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.48MiB/s]\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 140kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:42<00:00, 11.7MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 4.37MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 1.37MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.45MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from previous_chapters import (\n",
        "    generate_text_simple,\n",
        "    text_to_token_ids,\n",
        "    token_ids_to_text\n",
        ")\n",
        "\n",
        "text1 = \"Every effort moves you\"\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model = model,\n",
        "    idx = text_to_token_ids(text1, tokenizer),\n",
        "    max_new_tokens = 15,\n",
        "    context_size = BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr8Pp6bmclRU",
        "outputId": "26372861-0e12-4aa0-b21d-28e624afc812"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every effort moves you forward.\n",
            "\n",
            "The first step is to understand the importance of your work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad=False"
      ],
      "metadata": {
        "id": "Pcrb03sKg0r8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.out_head = torch.nn.Linear(BASE_CONFIG[\"emb_dim\"], 30)"
      ],
      "metadata": {
        "id": "5KN4dxyViFk3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf-dv_z-iWAc",
        "outputId": "b6b30752-2409-455e-ec97-2d801402464d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=30, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.trf_blocks[-1].parameters():\n",
        "  param.requires_grad=True\n",
        "\n",
        "for param in model.final_norm.parameters():\n",
        "  param.requires_grad=True"
      ],
      "metadata": {
        "id": "qNUaMGtliXXD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(\"Do you have time\")\n",
        "inputs = torch.tensor(inputs).unsqueeze(0)\n",
        "print(\"Inputs:\", inputs)\n",
        "print(\"Inputs dimensions\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTYvTyDdjLPT",
        "outputId": "c8adb937-564f-40df-fb55-b31facbec09d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: tensor([[5211,  345,  423,  640]])\n",
            "Inputs dimensions torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  outputs = model(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74ouoCl8j53B",
        "outputId": "303e7e45-c57f-46d2-d45f-c8aced27680e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.3403e+00,  1.6740e+00, -2.1391e+00, -1.5196e+00, -2.2827e+00,\n",
            "          -1.0842e+00,  1.0958e+00, -1.2350e+00,  1.4256e+00, -5.3516e-01,\n",
            "           1.9268e+00, -2.0557e+00, -3.3562e+00,  2.4921e+00, -2.8471e+00,\n",
            "          -5.0436e-01,  4.8762e-01, -3.8219e+00, -8.4622e-01,  1.5835e+00,\n",
            "          -8.1541e-01,  1.0916e+00,  1.5297e-01,  9.6537e-01, -2.9998e+00,\n",
            "          -1.9366e-01,  1.0422e+00,  1.8858e-01, -6.6791e-01, -2.2947e-01],\n",
            "         [-7.4813e+00,  5.6588e+00, -9.0387e+00, -3.9087e+00, -1.2606e+01,\n",
            "          -3.9699e+00,  2.9874e+00, -6.2640e+00,  7.4400e+00, -1.9896e+00,\n",
            "           1.2141e+01, -3.5960e+00, -1.0531e+01,  5.4487e+00, -7.1550e+00,\n",
            "           1.3336e+00,  3.4678e-01, -1.6510e+01, -5.8970e+00,  5.3332e+00,\n",
            "          -3.3368e+00,  5.4462e+00, -1.8515e+00,  8.6037e+00, -1.3210e+01,\n",
            "          -4.3487e+00,  3.6808e+00, -6.6142e-01, -4.5374e+00,  2.7694e+00],\n",
            "         [-6.9446e+00,  5.6580e+00, -7.3395e+00, -3.0708e+00, -9.0102e+00,\n",
            "          -4.2097e+00,  2.6408e+00, -5.4318e+00,  4.4718e+00, -3.3083e+00,\n",
            "           8.8275e+00, -3.5722e+00, -8.3022e+00,  4.8959e+00, -6.2047e+00,\n",
            "           1.2574e+00, -6.8048e-01, -1.3389e+01, -5.0322e+00,  4.2302e+00,\n",
            "          -9.8181e-01,  4.3566e+00, -2.7413e+00,  7.1674e+00, -1.0800e+01,\n",
            "          -3.5424e+00,  3.1056e+00, -2.4101e-01, -3.8836e+00,  1.0014e+00],\n",
            "         [-4.0886e+00,  4.5591e+00, -5.7935e+00, -2.7696e+00, -6.1608e+00,\n",
            "          -4.1616e+00,  2.8890e+00, -4.0644e+00,  2.3583e+00, -3.1705e+00,\n",
            "           6.1427e+00, -3.5661e+00, -8.0942e+00,  5.5738e+00, -5.9259e+00,\n",
            "           7.2265e-01,  7.5378e-01, -9.7794e+00, -2.7614e+00,  3.6197e+00,\n",
            "          -3.4055e-01,  3.1777e+00, -1.7628e+00,  3.9269e+00, -7.6645e+00,\n",
            "          -8.6025e-02,  3.6762e+00,  1.4327e-04, -4.0651e+00, -5.3126e-01]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = outputs[:,-1,:]"
      ],
      "metadata": {
        "id": "-xPv3RRAznvf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probas = torch.softmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "UK0la4Ty2vVQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8pz_qYX2wRX",
        "outputId": "df43c964-82c9-43c9-fe2c-d7e945bbb8e6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.6561e-05, 9.4345e-02, 3.0107e-06, 6.1934e-05, 2.0852e-06, 1.5395e-05,\n",
              "         1.7760e-02, 1.6966e-05, 1.0446e-02, 4.1479e-05, 4.5970e-01, 2.7926e-05,\n",
              "         3.0164e-07, 2.6026e-01, 2.6372e-06, 2.0351e-03, 2.0995e-03, 5.5927e-08,\n",
              "         6.2442e-05, 3.6876e-02, 7.0282e-04, 2.3704e-02, 1.6950e-04, 5.0138e-02,\n",
              "         4.6355e-07, 9.0654e-04, 3.9023e-02, 9.8812e-04, 1.6955e-05, 5.8080e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    total_batches = len(data_loader) if num_batches is None else min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i >= total_batches:\n",
        "            break\n",
        "        # Move data to device\n",
        "        input_batch = input_batch.to(device)\n",
        "        target_batch = target_batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_batch)[:, -1, :]\n",
        "        predicted_labels = torch.argmax(outputs, dim=-1)\n",
        "        num_examples += predicted_labels.shape[0]\n",
        "        correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "    return correct_predictions / num_examples\n"
      ],
      "metadata": {
        "id": "UcuWqH9MqW7X"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMGIKVJ147hG",
        "outputId": "a2eec51c-4598-4368-a075-47aed337c7d5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a007dde6d10>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)"
      ],
      "metadata": {
        "id": "5FoiyaVl5CLi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWbtGWJl5faM",
        "outputId": "8d3ac615-39ce-4045-9cbb-1439275cd8e0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.012422360248447204"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_accuracy = calc_accuracy_loader(validation_loader, model, device)\n",
        "validation_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiKmP9eN50ip",
        "outputId": "92a694fb-0c17-4b21-d5c7-beb8fcfb33ed"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01358695652173913"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "  logits = model(input_batch)[:,-1,:]\n",
        "  loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "HeJn22GO6CN3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  total_loss = 0\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss/num_batches"
      ],
      "metadata": {
        "id": "0UDSoxrN64tV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = calc_loss_loader(train_loader, model, device)\n",
        "train_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "otuFIA_F76BE",
        "outputId": "91e164c8-375b-4dd1-89f6-c388e8ec55a4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-c6c05efc607c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-f87d6ed84be8>\u001b[0m in \u001b[0;36mcalc_loss_loader\u001b[0;34m(data_loader, model, device, num_batches)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-736d36b3fcb4>\u001b[0m in \u001b[0;36mcalc_loss_batch\u001b[0;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine tuning the model**"
      ],
      "metadata": {
        "id": "5nz5KNWh8IZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier_simple(model, train_loader, validation_loader,\n",
        "                            optimizer, device, num_epochs, eval_freq, eval_iter):\n",
        "  train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "  examples_seen, global_step = 0, -1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      examples_seen += input_batch.shape[0]\n",
        "      global_step += 1\n",
        "\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model, train_loader, validation_loader, device, eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "              f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "      train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "      train_accs.append(train_accuracy)\n",
        "      val_accuracy = calc_accuracy_loader(validation_loader, model, device, num_batches=eval_iter)\n",
        "      val_accs.append(val_accuracy)\n",
        "      print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "      print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "  return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ],
      "metadata": {
        "id": "LoNd8zfS8Drn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, validation_loader, device, eval_iter):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "    val_loss = calc_loss_loader(validation_loader, model, device, num_batches=eval_iter)\n",
        "  model.train()\n",
        "  return train_loss, val_loss"
      ],
      "metadata": {
        "id": "NQd-pwfICJiD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    model = model,\n",
        "    train_loader = train_loader,\n",
        "    validation_loader = validation_loader,\n",
        "    optimizer = optimizer,\n",
        "    device = device,\n",
        "    num_epochs = num_epochs,\n",
        "    eval_freq = 50,\n",
        "    eval_iter = 5\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total training time: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "RKcqBpymDNNo",
        "outputId": "7c4fc048-a9ac-40e7-ef36-da64788d95f4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-7b20cf88727e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oauUDvPrm873",
        "outputId": "5f200273-c638-443c-c06f-b4e98b256dff"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare inputs to the model\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    supported_context_length = model.pos_emb.weight.shape[0]\n",
        "    # Note: In the book, this was originally written as pos_emb.weight.shape[1] by mistake\n",
        "    # It didn't break the code but would have caused unnecessary truncation (to 768 instead of 1024)\n",
        "\n",
        "    # Truncate sequences if they too long\n",
        "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "\n",
        "    # Pad sequences to the longest sequence\n",
        "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Return the classified result\n",
        "    return predicted_label"
      ],
      "metadata": {
        "id": "FRUODlkKJuDo"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example call:\n",
        "result = classify_review(\n",
        "    text=\"floral dress for women\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    max_length=train_dataset.max_length,\n",
        "    pad_token_id=50256\n",
        ")\n",
        "print(result)  # e.g., 'shoes_a12'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZprXgHSrkd1",
        "outputId": "882e3c7d-776e-4d92-c996-6d659342edfc"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your category_map as provided\n",
        "category_map = {\n",
        "    'shoes_a12': 0,\n",
        "    'makeup_a30': 1,\n",
        "    'bottoms_a3': 2,\n",
        "    'tops_a2': 3,\n",
        "    'dresses_a1': 4,\n",
        "    'underwear_lingerie_a7': 5,\n",
        "    'fragrance_a31': 6,\n",
        "    'skin_body_treatment_a32': 7,\n",
        "    'outerwear_a11': 8,\n",
        "    'sleepwear_a9': 9,\n",
        "    'jewelry_a14': 10,\n",
        "    'bags_a17': 11,\n",
        "    'hosiery_a16': 12,\n",
        "    'baby_accessories_a28': 13,\n",
        "    'swimwear_a8': 14,\n",
        "    'home_a35': 15,\n",
        "    'hair_care_a33': 16,\n",
        "    'eyewear_a19': 17,\n",
        "    'hair_accessories_a20': 18,\n",
        "    'headwear_a18': 19,\n",
        "    'jumpsuits_coveralls_a4': 20,\n",
        "    'personal_care_accessories_a34': 21,\n",
        "    'belts__braces_a26': 22,\n",
        "    'jacket_sportcoat_mg12': 23,\n",
        "    'small_leather_goods_a23': 24,\n",
        "    'food_a38': 25,\n",
        "    'suits_sets_wardrobers_mg5': 26,\n",
        "    'stationery_giftwrap_a36': 27,\n",
        "    'shoe_care_a13': 28,\n",
        "    'toys_games_a37': 29\n",
        "}\n",
        "\n",
        "# Invert the category_map to map indices back to category names\n",
        "index_to_category = {v: k for k, v in category_map.items()}\n",
        "\n",
        "# Example usage\n",
        "text = \"lipstick cetics\"\n",
        "predicted_label = classify_review(text, model, tokenizer, device, max_length=64, pad_token_id=50256)\n",
        "\n",
        "# If condition to print the category name\n",
        "if predicted_label in index_to_category:\n",
        "    print(f\"Predicted category: {index_to_category[predicted_label]}\")\n",
        "else:\n",
        "    print(f\"Unknown category index: {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOVMt1dPr2Yq",
        "outputId": "0533e760-a80f-4108-a94e-275fa5525cd1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted category: underwear_lingerie_a7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lmsI5oN4r-60"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}